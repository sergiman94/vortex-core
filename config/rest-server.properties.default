# The data store type.
backend=memory

# The all data store type.
backends=[memory]

# The config file path of computer job.
computer.config=./conf/computer.yaml

# The max cache size(items) of edge cache.
edge.cache_capacity=1000000

# The expiration time in seconds of edge cache.
edge.cache_expire=600

# The type of edge cache, allowed values are [l1, l2].
edge.cache_type=l2

# The max size(items) of edges(uncommitted) in transaction.
edge.tx_capacity=10000

# Gremlin entrance to create graph.
gremlin.graph=com.vortex.vortexdb.VortexFactory

# The implementation type of collections used in oltp algorithm.
oltp.collection_type=EC

# The min depth to enable concurrent oltp algorithm.
oltp.concurrent_depth=10

# Thread number to concurrently execute oltp algorithm.
oltp.concurrent_threads=10

# The size of each batch when querying by batch.
query.batch_size=1000

# Whether to ignore invalid data of vertex or edge.
query.ignore_invalid_data=true

# The maximum number of intermediate results to intersect indexes when querying by multiple single index properties.
query.index_intersect_threshold=1000

# Whether to optimize aggregate query(like count) by index.
query.optimize_aggregate_by_index=false

# The size of each page when querying by paging.
query.page_size=500

# The maximum number of edges in ramtable, include OUT and IN edges.
query.ramtable_edges_capacity=20000000

# Whether to enable ramtable for query of adjacent edges.
query.ramtable_enable=false

# The maximum number of vertices in ramtable, generally the largest vertex id is used as capacity.
query.ramtable_vertices_capacity=10000000

# The apply batch size to trigger disruptor event handler.
raft.apply_batch=1

# The thread number used to apply task to bakcend.
raft.backend_threads=8

# Timeout in milliseconds to launch a round of election.
raft.election_timeout=10000

# The peerid of current raft node.
raft.endpoint=127.0.0.1:8281

# The peers of current raft group.
raft.group_peers=127.0.0.1:8281,127.0.0.1:8282,127.0.0.1:8283

# Whether the backend storage works in raft mode.
raft.mode=false

# The log path of current raft node.
raft.path=./raftlog

# The timeout in second when publish event into disruptor.
raft.queue_publish_timeout=60

# The disruptor buffers size for jraft RaftNode, StateMachine and LogManager.
raft.queue_size=16384

# The thread number used to execute reading index.
raft.read_index_threads=8

# The linearizability of read strategy.
raft.read_strategy=ReadOnlyLeaseBased

# The ChannelOutboundBuffer's high water mark of netty, only when buffer size exceed this size, the method ChannelOutboundBuffer.isWritable() will return false, it means that the downstream pressure is too great to process the request or network is very congestion, upstream needs to limit rate at this time.
raft.rpc_buf_high_water_mark=20971520

# The ChannelOutboundBuffer's low water mark of netty, when buffer size less than this size, the method ChannelOutboundBuffer.isWritable() will return true, it means that low downstream pressure or good network.
raft.rpc_buf_low_water_mark=10485760

# The rpc connect timeout for jraft rpc.
raft.rpc_connect_timeout=5000

# The rpc threads for jraft RPC layer
raft.rpc_threads=80

# The rpc timeout for jraft rpc.
raft.rpc_timeout=60000

# Whether to use linearly consistent read.
raft.safe_read=false

# The interval in seconds to trigger snapshot save.
raft.snapshot_interval=3600

# Whether to use replicator line, when turned on it multiple logs can be sent in parallel, and the next log doesn't have to wait for the ack message of the current log to be sent.
raft.use_replicator_pipeline=true

# Whether to use snapshot.
raft.use_snapshot=true

# The max rate(times/s) to execute query of vertices/edges.
rate_limit.read=0

# The max rate(items/s) to add/update/delete vertices/edges.
rate_limit.write=0

# The max cache size(items) of schema cache.
schema.cache_capacity=10000

# The regex specified the illegal format for schema name.
schema.illegal_name_regex=.*\s+$|~.*

# Choose a text analyzer for searching the vertex/edge properties, available type are [word, ansj, hanlp, smartcn, jieba, jcseg, mmseg4j, ikanalyzer].
search.text_analyzer=ikanalyzer

# Specify the mode for the text analyzer, the available mode of analyzer are {word: [MaximumMatching, ReverseMaximumMatching, MinimumMatching, ReverseMinimumMatching, BidirectionalMaximumMatching, BidirectionalMinimumMatching, BidirectionalMaximumMinimumMatching, FullSegmentation, MinimalWordCount, MaxNgramScore, PureEnglish], ansj: [BaseAnalysis, IndexAnalysis, ToAnalysis, NlpAnalysis], hanlp: [standard, nlp, index, nShort, shortest, speed], smartcn: [], jieba: [SEARCH, INDEX], jcseg: [Simple, Complex], mmseg4j: [Simple, Complex, MaxWord], ikanalyzer: [smart, max_word]}.
search.text_analyzer_mode=smart

# The serializer for backend store, like: text/binary/cassandra.
serializer=text

# The datacenter id of snowflake id generator.
snowflake.datecenter_id=0

# Whether to force the snowflake long id to be a string.
snowflake.force_string=false

# The worker id of snowflake id generator.
snowflake.worker_id=0

# The database name like Cassandra Keyspace.
store=vortex

# The interval in seconds for detecting connections, if the idle time of a connection exceeds this value, detect it and reconnect if needed before using, value 0 means detecting every time.
store.connection_detect_interval=600

# The graph table name, which store vertex, edge and property.
store.graph=g

# The schema table name, which store meta data.
store.schema=m

# The system table name, which store system data.
store.system=s

# The job input size limit in bytes.
task.input_size_limit=16777216

# The job result size limit in bytes.
task.result_size_limit=16777216

# Whether to delete schema or expired data synchronously.
task.sync_deletion=false

# The batch size used to delete expired data.
task.ttl_delete_batch=1

# Timeout in seconds for waiting for the task to complete, such as when truncating or clearing the backend.
task.wait_timeout=10

# The max cache size(items) of vertex cache.
vertex.cache_capacity=10000000

# The expiration time in seconds of vertex cache.
vertex.cache_expire=600

# The type of vertex cache, allowed values are [l1, l2].
vertex.cache_type=l2

# Whether to check the adjacent vertices of edges exist.
vertex.check_adjacent_vertex_exist=false

# Whether to check the vertices exist for those using customized id strategy.
vertex.check_customized_id_exist=false

# The default vertex label.
vertex.default_label=vertex

# Whether to encode number value of primary key in vertex id.
vertex.encode_primary_key_number=true

# Whether to lazy load adjacent vertices of edges.
vertex.lazy_load_adjacent_vertex=true

# Whether to enable the mode to commit part of edges of vertex, enabled if commit size > 0, 0 meas disabled.
vertex.part_edge_commit_size=5000

# Whether remove left index at overwrite.
vertex.remove_left_index_at_overwrite=false

# The max size(items) of vertices(uncommitted) in transaction.
vertex.tx_capacity=10000

# The data store type.
backend=memory

# The all data store type.
backends=[memory]

# The config file path of computer job.
computer.config=./conf/computer.yaml

# The max cache size(items) of edge cache.
edge.cache_capacity=1000000

# The expiration time in seconds of edge cache.
edge.cache_expire=600

# The type of edge cache, allowed values are [l1, l2].
edge.cache_type=l2

# The max size(items) of edges(uncommitted) in transaction.
edge.tx_capacity=10000

# Gremlin entrance to create graph.
gremlin.graph=com.vortex.vortexdb.VortexFactory

# The implementation type of collections used in oltp algorithm.
oltp.collection_type=EC

# The min depth to enable concurrent oltp algorithm.
oltp.concurrent_depth=10

# Thread number to concurrently execute oltp algorithm.
oltp.concurrent_threads=10

# The size of each batch when querying by batch.
query.batch_size=1000

# Whether to ignore invalid data of vertex or edge.
query.ignore_invalid_data=true

# The maximum number of intermediate results to intersect indexes when querying by multiple single index properties.
query.index_intersect_threshold=1000

# Whether to optimize aggregate query(like count) by index.
query.optimize_aggregate_by_index=false

# The size of each page when querying by paging.
query.page_size=500

# The maximum number of edges in ramtable, include OUT and IN edges.
query.ramtable_edges_capacity=20000000

# Whether to enable ramtable for query of adjacent edges.
query.ramtable_enable=false

# The maximum number of vertices in ramtable, generally the largest vertex id is used as capacity.
query.ramtable_vertices_capacity=10000000

# The apply batch size to trigger disruptor event handler.
raft.apply_batch=1

# The thread number used to apply task to bakcend.
raft.backend_threads=8

# Timeout in milliseconds to launch a round of election.
raft.election_timeout=10000

# The peerid of current raft node.
raft.endpoint=127.0.0.1:8281

# The peers of current raft group.
raft.group_peers=127.0.0.1:8281,127.0.0.1:8282,127.0.0.1:8283

# Whether the backend storage works in raft mode.
raft.mode=false

# The log path of current raft node.
raft.path=./raftlog

# The timeout in second when publish event into disruptor.
raft.queue_publish_timeout=60

# The disruptor buffers size for jraft RaftNode, StateMachine and LogManager.
raft.queue_size=16384

# The thread number used to execute reading index.
raft.read_index_threads=8

# The linearizability of read strategy.
raft.read_strategy=ReadOnlyLeaseBased

# The ChannelOutboundBuffer's high water mark of netty, only when buffer size exceed this size, the method ChannelOutboundBuffer.isWritable() will return false, it means that the downstream pressure is too great to process the request or network is very congestion, upstream needs to limit rate at this time.
raft.rpc_buf_high_water_mark=20971520

# The ChannelOutboundBuffer's low water mark of netty, when buffer size less than this size, the method ChannelOutboundBuffer.isWritable() will return true, it means that low downstream pressure or good network.
raft.rpc_buf_low_water_mark=10485760

# The rpc connect timeout for jraft rpc.
raft.rpc_connect_timeout=5000

# The rpc threads for jraft RPC layer
raft.rpc_threads=80

# The rpc timeout for jraft rpc.
raft.rpc_timeout=60000

# Whether to use linearly consistent read.
raft.safe_read=false

# The interval in seconds to trigger snapshot save.
raft.snapshot_interval=3600

# Whether to use replicator line, when turned on it multiple logs can be sent in parallel, and the next log doesn't have to wait for the ack message of the current log to be sent.
raft.use_replicator_pipeline=true

# Whether to use snapshot.
raft.use_snapshot=true

# The max rate(times/s) to execute query of vertices/edges.
rate_limit.read=0

# The max rate(items/s) to add/update/delete vertices/edges.
rate_limit.write=0

# The max cache size(items) of schema cache.
schema.cache_capacity=10000

# The regex specified the illegal format for schema name.
schema.illegal_name_regex=.*\s+$|~.*

# Choose a text analyzer for searching the vertex/edge properties, available type are [word, ansj, hanlp, smartcn, jieba, jcseg, mmseg4j, ikanalyzer].
search.text_analyzer=ikanalyzer

# Specify the mode for the text analyzer, the available mode of analyzer are {word: [MaximumMatching, ReverseMaximumMatching, MinimumMatching, ReverseMinimumMatching, BidirectionalMaximumMatching, BidirectionalMinimumMatching, BidirectionalMaximumMinimumMatching, FullSegmentation, MinimalWordCount, MaxNgramScore, PureEnglish], ansj: [BaseAnalysis, IndexAnalysis, ToAnalysis, NlpAnalysis], hanlp: [standard, nlp, index, nShort, shortest, speed], smartcn: [], jieba: [SEARCH, INDEX], jcseg: [Simple, Complex], mmseg4j: [Simple, Complex, MaxWord], ikanalyzer: [smart, max_word]}.
search.text_analyzer_mode=smart

# The serializer for backend store, like: text/binary/cassandra.
serializer=text

# The datacenter id of snowflake id generator.
snowflake.datecenter_id=0

# Whether to force the snowflake long id to be a string.
snowflake.force_string=false

# The worker id of snowflake id generator.
snowflake.worker_id=0

# The database name like Cassandra Keyspace.
store=vortex

# The interval in seconds for detecting connections, if the idle time of a connection exceeds this value, detect it and reconnect if needed before using, value 0 means detecting every time.
store.connection_detect_interval=600

# The graph table name, which store vertex, edge and property.
store.graph=g

# The schema table name, which store meta data.
store.schema=m

# The system table name, which store system data.
store.system=s

# The job input size limit in bytes.
task.input_size_limit=16777216

# The job result size limit in bytes.
task.result_size_limit=16777216

# Whether to delete schema or expired data synchronously.
task.sync_deletion=false

# The batch size used to delete expired data.
task.ttl_delete_batch=1

# Timeout in seconds for waiting for the task to complete, such as when truncating or clearing the backend.
task.wait_timeout=10

# The max cache size(items) of vertex cache.
vertex.cache_capacity=10000000

# The expiration time in seconds of vertex cache.
vertex.cache_expire=600

# The type of vertex cache, allowed values are [l1, l2].
vertex.cache_type=l2

# Whether to check the adjacent vertices of edges exist.
vertex.check_adjacent_vertex_exist=false

# Whether to check the vertices exist for those using customized id strategy.
vertex.check_customized_id_exist=false

# The default vertex label.
vertex.default_label=vertex

# Whether to encode number value of primary key in vertex id.
vertex.encode_primary_key_number=true

# Whether to lazy load adjacent vertices of edges.
vertex.lazy_load_adjacent_vertex=true

# Whether to enable the mode to commit part of edges of vertex, enabled if commit size > 0, 0 meas disabled.
vertex.part_edge_commit_size=5000

# Whether remove left index at overwrite.
vertex.remove_left_index_at_overwrite=false

# The max size(items) of vertices(uncommitted) in transaction.
vertex.tx_capacity=10000

